{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "280ab6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "headers = {'default':{'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36',\"Referer\": \"https://www.google.com\"},\"alternative\":{'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36',\"Referer\": \"https://www.bing.com\"}}\n",
    "from bs4 import BeautifulSoup\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures as pf\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import MinMaxScaler as mms\n",
    "from sklearn.ensemble import RandomForestRegressor as rfr\n",
    "from sklearn.linear_model import Ridge\n",
    "import datetime\n",
    "import jdatetime\n",
    "import math\n",
    "import jalali_pandas\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69545a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices=[]\n",
    "kms=[]\n",
    "years=[]\n",
    "city=[]\n",
    "dates=[]\n",
    "colors=[]\n",
    "body=[]\n",
    "types=[]\n",
    "urls=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dfe8c7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divar(url):\n",
    "    cities=['tehran','karaj','qazvin','tabriz']\n",
    "    cities=['tehran','karaj','tabriz','isfahan']\n",
    "    cities=['qazvin']\n",
    "    prices=[]\n",
    "    kms=[]\n",
    "    years=[]\n",
    "    city=[]\n",
    "    dates=[]\n",
    "    colors=[]\n",
    "    body=[]\n",
    "    types=[]\n",
    "    urls=[]\n",
    "    today=['دقایقی' , 'لحظاتی' , 'ساعت','نیم','ربع']\n",
    "    for c in cities:\n",
    "        url=f'https://divar.ir/s/{c}/{url}'\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(url)\n",
    "        # Get the initial HTML source\n",
    "        html = driver.page_source\n",
    "\n",
    "        # Parse the HTML with BeautifulSoup\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        # Print the title of the website\n",
    "        try:\n",
    "            print(soup.title.text)\n",
    "        except AttributeError:\n",
    "                pass\n",
    "        # Define a function to scroll down the page\n",
    "        def scroll_down():\n",
    "            # Get the height of the document\n",
    "            height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            # Scroll to the bottom of the page\n",
    "            driver.execute_script(f\"window.scrollTo(0, {height});\")\n",
    "            # Wait for the new content to load\n",
    "            time.sleep(3)\n",
    "\n",
    "        # Scroll down the page 5 times\n",
    "        for i in range(1):\n",
    "\n",
    "\n",
    "            # Get the updated HTML source\n",
    "            html = driver.page_source\n",
    "\n",
    "            # Parse the HTML with BeautifulSoup\n",
    "            soup=((BeautifulSoup(html, \"html.parser\")))\n",
    "            results=soup.find_all('div',class_='kt-post-card__description')\n",
    "            for i in range(len(results)):\n",
    "                if i%2==0:\n",
    "                    kms.append(unidecode((((results[i].get_text()).replace('کیلومتر','')).strip()).replace(',','')))\n",
    "                    if c=='tehran':\n",
    "                        city.append('Tehran')\n",
    "                    elif c=='karaj':\n",
    "                        city.append('Karaj')\n",
    "                    elif c=='qazvin':\n",
    "                        city.append('Qazvin')\n",
    "                    elif c=='isfahan':\n",
    "                        city.append('Isfahan')\n",
    "                    elif c=='tabriz':\n",
    "                        city.append('Tabriz')\n",
    "                    else:\n",
    "                        city.append('N/A')\n",
    "                else:\n",
    "                    prices.append(unidecode((((results[i].get_text()).replace('تومان','')).strip()).replace(',','')))\n",
    "            results=soup.find_all(\"div\",class_='kt-post-card__bottom')\n",
    "            for i in range(len(results)):\n",
    "                if any(x in today for x in (results[i].get_text()).split()):\n",
    "                    #jdatetime.date.fromgregorian(now.date())\n",
    "                    dates.append(str(jdatetime.date.today()))\n",
    "                elif 'دیروز' in ((results[i].get_text()).split()):\n",
    "                    dates.append(str(jdatetime.date.today()-datetime.timedelta(days=1)))\n",
    "                elif 'پریروز' in ((results[i].get_text()).split()):\n",
    "                    dates.append(str(jdatetime.date.today()-datetime.timedelta(days=2)))\n",
    "                elif 'روز' in ((results[i].get_text()).split()):\n",
    "                    dates.append(str(jdatetime.date.today()-datetime.timedelta(days=(int(unidecode(results[i].get_text())[0])))))\n",
    "                elif (('هفته') in ((results[i].get_text()).split())) or (('هفتهٔ') in ((results[i].get_text()).split())):\n",
    "                    if (results[i].get_text())[5] == ' ':\n",
    "                        dates.append(str(jdatetime.date.today()-datetime.timedelta(days=(7))))\n",
    "                    else:\n",
    "                        dates.append(str(jdatetime.date.today()-datetime.timedelta(days=(7*int(unidecode(results[i].get_text())[0])))))\n",
    "                else:\n",
    "                    if i-1!=-1:\n",
    "                        dates.append(dates[i-1])\n",
    "                    else:\n",
    "                        dates.append(str(jdatetime.date.today()))\n",
    "            results=soup.find_all(\"div\",class_='post-card-item-af972 kt-col-6-bee95 kt-col-xxl-4-e9d46')\n",
    "            for i in range(len(results)):\n",
    "                a_tag=results[i].find('a')\n",
    "                try:\n",
    "\n",
    "                    urls.append('https://www.divar.ir'+a_tag['href'])\n",
    "                except TypeError:\n",
    "                    continue\n",
    "            scroll_down()\n",
    "            time.sleep(3)\n",
    "        # Print the number of items on the page\n",
    "        print(len(soup.find_all(\"div\", class_=\"item\")))\n",
    "\n",
    "        # Close the driver\n",
    "        driver.close()\n",
    "    colors=[]\n",
    "    body=[]\n",
    "    types=[]\n",
    "    years=[]\n",
    "    description=[]\n",
    "    for i in urls:\n",
    "    #     if i==urls[1]:\n",
    "    #         break\n",
    "        #time.sleep(1)\n",
    "        try:\n",
    "            response = requests.get(i, headers=headers['default'])\n",
    "            if response.status_code==200:\n",
    "                pass\n",
    "            elif response.status_code==404:\n",
    "                colors.append('err')\n",
    "                body.append('err')\n",
    "                types.append('err')\n",
    "                years.append('err')\n",
    "                description.append('err')\n",
    "                continue\n",
    "            else:\n",
    "                print(f'Failed. The status code is {response.status_code}\\n Retrying...')\n",
    "                time.sleep(2)\n",
    "                response = requests.get(i, headers=headers['alternative'])\n",
    "                if response.status_code==200:\n",
    "                    pass\n",
    "                else:\n",
    "                    print(f'Failed. The status code is {response.status_code}')\n",
    "        except ChunkedEncodingError:\n",
    "            print('ChunkedEncodingError!/n Retrying...')\n",
    "            time.sleep(2)\n",
    "            response = requests.get(i, headers=headers['alternative'])\n",
    "            if response.status_code==200:\n",
    "                pass\n",
    "            else:\n",
    "                print(f'Failed. The ChunkedEncodingError Presists or may be connection error with the code {response.status_code} ')\n",
    "        except InvalidChunkLength:\n",
    "            print('ChunkedEncodingError!/n Retrying...')\n",
    "            time.sleep(2)\n",
    "            response = requests.get(i, headers=headers['alternative'])\n",
    "            if response.status_code==200:\n",
    "                pass\n",
    "            else:\n",
    "                print(f'Failed. The ChunkedEncodingError Presists or may be connection error with the code {response.status_code} ')\n",
    "    #     driver = webdriver.Chrome()\n",
    "    #     driver.get(f'https://www.divar.ir{i}')\n",
    "    #     # Get the initial HTML source\n",
    "    #     html = driver.page_source\n",
    "        internal_soup=BeautifulSoup(response.content,\"html.parser\")\n",
    "        internal_soup=str(internal_soup)\n",
    "    #     #results=internal_soup.find_all('span',class_=\"kt-group-row-item__value\")\n",
    "    #     driver.close()\n",
    "        try:\n",
    "\n",
    "            #match = re.search(pattern, internal_soup)\n",
    "            pass\n",
    "\n",
    "        except IndexError :\n",
    "            colors.append('err')\n",
    "            body.append('err')\n",
    "            types.append('err')\n",
    "            years.append('err')\n",
    "            description.append('err')\n",
    "            continue\n",
    "        pattern = r'\"title\":\"رنگ\",\"value\":\"([^\"]*)\"'\n",
    "        match = re.search(pattern, internal_soup)\n",
    "        try:\n",
    "            colors.append((match.group(1)).replace('\\u200c',' '))\n",
    "        except AttributeError :\n",
    "            colors.append('err')\n",
    "            body.append('err')\n",
    "            types.append('err')\n",
    "            years.append('err')\n",
    "            description.append('err')\n",
    "            continue\n",
    "        pattern = r'\"title\":\"مدل \\(سال تولید\\)\",\"value\":\"([^\"]*)\"'\n",
    "        match = re.search(pattern, internal_soup)\n",
    "        years.append(unidecode(match.group(1)))\n",
    "        pattern = r'\"title\":\"برند و تیپ\",\"value\":\"([^\"]*)\"'\n",
    "        match = re.search(pattern, internal_soup)\n",
    "        types.append((match.group(1)))\n",
    "        pattern = r'\"title\":\"وضعیت بدنه\",\"value\":\"([^\"]*)\"'\n",
    "        match = re.search(pattern, internal_soup)\n",
    "        try:\n",
    "            body.append((match.group(1)).replace('\\u200c',' '))\n",
    "        except AttributeError:\n",
    "            body.append('N/A')\n",
    "        try:\n",
    "            pattern = r'\"DESCRIPTION_ROW\",\"children\":\"([^\"]*)\"'\n",
    "            match = re.findall(pattern, internal_soup)\n",
    "            description.append(match[2])\n",
    "        except AttributeError:\n",
    "            description.append('N/A')\n",
    "    counter=0\n",
    "    for i in range(len(urls)):\n",
    "        if colors[i-counter]=='err':\n",
    "            del types[i-counter]\n",
    "            del years[i-counter]\n",
    "            del colors[i-counter]\n",
    "            del prices[i-counter]\n",
    "            del kms[i-counter]\n",
    "            del body[i-counter]\n",
    "            del dates[i-counter]\n",
    "            del city[i-counter]\n",
    "            del urls[i-counter]\n",
    "            del description[i-counter]\n",
    "            counter=counter+1\n",
    "            print(1)\n",
    "            continue\n",
    "        if ((prices[i-counter].count('1'))>=4) or ((prices[i-counter].count('6'))>=4) or ((prices[i-counter].count('5'))>=4) or ((prices[i-counter].count('4'))>=4) or ((prices[i-counter].count('3'))>=4)or ((prices[i-counter].count('2'))>=4):\n",
    "            del types[i-counter]\n",
    "            del years[i-counter]\n",
    "            del colors[i-counter]\n",
    "            del prices[i-counter]\n",
    "            del kms[i-counter]\n",
    "            del body[i-counter]\n",
    "            del dates[i-counter]\n",
    "            del city[i-counter]\n",
    "            del urls[i-counter]\n",
    "            del description[i-counter]\n",
    "            counter=counter+1\n",
    "            print(2)\n",
    "            continue\n",
    "        try:\n",
    "            int(years[i-counter])\n",
    "        except ValueError:\n",
    "            del types[i-counter]\n",
    "            del years[i-counter]\n",
    "            del colors[i-counter]\n",
    "            del prices[i-counter]\n",
    "            del kms[i-counter]\n",
    "            del body[i-counter]\n",
    "            del dates[i-counter]\n",
    "            del city[i-counter]\n",
    "            del urls[i-counter]\n",
    "            del description[i-counter]\n",
    "            counter=counter+1\n",
    "            print(3)\n",
    "            continue\n",
    "        try:\n",
    "            int(prices[i-counter])\n",
    "        except ValueError:\n",
    "            del types[i-counter]\n",
    "            del years[i-counter]\n",
    "            del colors[i-counter]\n",
    "            del prices[i-counter]\n",
    "            del kms[i-counter]\n",
    "            del body[i-counter]\n",
    "            del dates[i-counter]\n",
    "            del city[i-counter]\n",
    "            del urls[i-counter]\n",
    "            del description[i-counter]\n",
    "            counter=counter+1\n",
    "            print(4)\n",
    "            continue\n",
    "        if urls[i-counter].count('حواله'):\n",
    "            del types[i-counter]\n",
    "            del years[i-counter]\n",
    "            del colors[i-counter]\n",
    "            del prices[i-counter]\n",
    "            del kms[i-counter]\n",
    "            del body[i-counter]\n",
    "            del dates[i-counter]\n",
    "            del city[i-counter]\n",
    "            del urls[i-counter]\n",
    "            del description[i-counter]\n",
    "            print(5)\n",
    "            counter=counter+1\n",
    "            continue\n",
    "        if description[i-counter].count('حواله'):\n",
    "            del types[i-counter]\n",
    "            del years[i-counter]\n",
    "            del colors[i-counter]\n",
    "            del prices[i-counter]\n",
    "            del kms[i-counter]\n",
    "            del body[i-counter]\n",
    "            del dates[i-counter]\n",
    "            del city[i-counter]\n",
    "            del urls[i-counter]\n",
    "            del description[i-counter]\n",
    "            print(6)\n",
    "            counter=counter+1\n",
    "            continue\n",
    "        if description[i-counter].count('پیش پرداخت'):\n",
    "            del types[i-counter]\n",
    "            del years[i-counter]\n",
    "            del colors[i-counter]\n",
    "            del prices[i-counter]\n",
    "            del kms[i-counter]\n",
    "            del body[i-counter]\n",
    "            del dates[i-counter]\n",
    "            del city[i-counter]\n",
    "            del urls[i-counter]\n",
    "            del description[i-counter]\n",
    "            print(7)\n",
    "            counter=counter+1\n",
    "            continue\n",
    "    print(len(prices),len(kms),len(dates),len(urls),len(city),len(years),len(colors),len(body),len(types),len(description))\n",
    "    colors=['سفید'if x=='سفید صدفی' else x for x in colors]\n",
    "    return prices,kms,years,city,dates,colors,body,types,urls\n",
    "def sheypoor(url):\n",
    "    prices=[]\n",
    "    kms=[]\n",
    "    years=[]\n",
    "    city=[]\n",
    "    dates=[]\n",
    "    colors=[]\n",
    "    body=[]\n",
    "    types=[]\n",
    "    urls=[]\n",
    "    today=['دقایقی' , 'لحظاتی' , 'ساعاتی','نیم','ربع']\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    # Get the initial HTML source\n",
    "    html = driver.page_source\n",
    "    soup=((BeautifulSoup(html, \"html.parser\")))\n",
    "    # Parse the HTML with BeautifulSoup\n",
    "\n",
    "    # Print the title of the website\n",
    "    #print(soup.title.text)\n",
    "\n",
    "    # Define a function to scroll down the page\n",
    "    def scroll_down():\n",
    "        # Get the height of the document\n",
    "        height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        # Scroll to the bottom of the page\n",
    "        driver.execute_script(f\"window.scrollTo(0, {height});\")\n",
    "        # Wait for the new content to load\n",
    "        time.sleep(2)\n",
    "    # Locate the button element by its ID\n",
    "    button = driver.find_element(\"class name\", \"aUnqO\")\n",
    "\n",
    "    # Click the button\n",
    "    button.click()\n",
    "    # Scroll down the page 5 times\n",
    "    for i in range(20):\n",
    "        html = driver.page_source\n",
    "        soup=((BeautifulSoup(html, \"html.parser\")))\n",
    "        results=soup.find_all('div',class_='YPVjY')\n",
    "        for i in range(len(results)):\n",
    "            try:\n",
    "                results[i].find_all('small',class_='-GCBo')[1].text.split()[0]\n",
    "                prices.append(unidecode(results[i].find('span',class_='J5s1-').text.replace(',','')))\n",
    "                urls.append(results[i].find('a')['href'])\n",
    "                city.append(results[i].find_all('small',class_='-GCBo')[0].text.split('،')[0])\n",
    "\n",
    "                if any(x in today for x in (results[i].find_all('small',class_='-GCBo')[1].text.split())):\n",
    "                    #jdatetime.date.fromgregorian(now.date())\n",
    "                    dates.append(str(jdatetime.date.today()))\n",
    "                elif 'روز' in (results[i].find_all('small',class_='-GCBo')[1].text.split()):\n",
    "                    dates.append(str(jdatetime.date.today()-datetime.timedelta(days=(int(unidecode(results[i].find_all('small',class_='-GCBo')[1].text.split()[0]))))))\n",
    "                else:\n",
    "                    dates.append(str(jdatetime.date.today()))\n",
    "\n",
    "            except (AttributeError,IndexError):\n",
    "                    print('error')\n",
    "                    continue\n",
    "        scroll_down()\n",
    "        time.sleep(3)\n",
    "    # Print the number of items on the page\n",
    "\n",
    "    # Close the driver\n",
    "    driver.close()\n",
    "    colors=[]\n",
    "    kms=[]\n",
    "    years=[]\n",
    "    types=[]\n",
    "    bodies=[]\n",
    "    for i in range(len(urls)):\n",
    "        response = requests.get(urls[i], headers=headers['default'])\n",
    "        if response.status_code==200:\n",
    "            pass\n",
    "        elif response.status_code==404:\n",
    "            continue\n",
    "        else:\n",
    "            print(f'Failed. The status code is {response.status_code}\\n Retrying...')\n",
    "            time.sleep(2)\n",
    "            response = requests.get(urls[i], headers=headers['alternative'])\n",
    "            if response.status_code==200:\n",
    "                pass\n",
    "            else:\n",
    "                print(f'Failed. The status code is {response.status_code}')\n",
    "        soup=BeautifulSoup(response.content,\"html.parser\")\n",
    "        soup=soup.find('div',class_='bWPjU')\n",
    "        if len(unidecode(soup.find_all(\"p\", class_=\"_874-x\")[1].text))==4:\n",
    "            years.append(unidecode(soup.find_all(\"p\", class_=\"_874-x\")[1].text))\n",
    "            try:\n",
    "                int(unidecode(soup.find_all(\"p\", class_=\"_874-x\")[2].text).replace(',',''))\n",
    "            except ValueError:\n",
    "                kms.append('100000')\n",
    "                types.append((soup.find_all(\"p\", class_=\"_874-x\")[2].text)) \n",
    "                colors.append((soup.find_all(\"p\", class_=\"_874-x\")[3].text))\n",
    "                bodies.append((soup.find_all(\"p\", class_=\"_874-x\")[6].text))\n",
    "                continue\n",
    "            kms.append(unidecode(soup.find_all(\"p\", class_=\"_874-x\")[2].text).replace(',',''))\n",
    "            types.append((soup.find_all(\"p\", class_=\"_874-x\")[3].text)) \n",
    "            colors.append((soup.find_all(\"p\", class_=\"_874-x\")[4].text))\n",
    "            bodies.append((soup.find_all(\"p\", class_=\"_874-x\")[7].text))  \n",
    "        else:\n",
    "            try :\n",
    "                int(unidecode(soup.find_all(\"p\", class_=\"_874-x\")[2].text).replace(',',''))\n",
    "            except ValueError:\n",
    "                years.append(unidecode(soup.find_all(\"p\", class_=\"_874-x\")[0].text))\n",
    "                kms.append('100000')\n",
    "                types.append((soup.find_all(\"p\", class_=\"_874-x\")[1].text)) \n",
    "                colors.append((soup.find_all(\"p\", class_=\"_874-x\")[2].text))\n",
    "                bodies.append((soup.find_all(\"p\", class_=\"_874-x\")[5].text))\n",
    "                continue\n",
    "            years.append(unidecode(soup.find_all(\"p\", class_=\"_874-x\")[0].text))\n",
    "            kms.append(unidecode(soup.find_all(\"p\", class_=\"_874-x\")[1].text).replace(',',''))\n",
    "            types.append((soup.find_all(\"p\", class_=\"_874-x\")[2].text)) \n",
    "            colors.append((soup.find_all(\"p\", class_=\"_874-x\")[3].text))\n",
    "            bodies.append((soup.find_all(\"p\", class_=\"_874-x\")[6].text))\n",
    "            counter=0\n",
    "        for i in range(len(urls)):\n",
    "            if urls[i-counter].count('حواله'):\n",
    "                del types[i-counter]\n",
    "                del years[i-counter]\n",
    "                del colors[i-counter]\n",
    "                del prices[i-counter]\n",
    "                del kms[i-counter]\n",
    "                del bodies[i-counter]\n",
    "                del dates[i-counter]\n",
    "                del city[i-counter]\n",
    "                del urls[i-counter]\n",
    "                counter=counter+1\n",
    "                continue\n",
    "            else:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aa8efb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "خرید و فروش و قیمت خودرو رنو تلیسمان در قزوین | دیوار\n",
      "0\n",
      "1 1 1 1 1 1 1 1 1 1\n"
     ]
    }
   ],
   "source": [
    "for i in [('car/renault/talisman')]:\n",
    "    divar_ads=divar(i)\n",
    "    prices=prices+divar_ads[0]\n",
    "    kms=kms+divar_ads[1]\n",
    "    years=years+divar_ads[2]\n",
    "    city=city+divar_ads[3]\n",
    "    dates=dates+divar_ads[4]\n",
    "    colors=colors+divar_ads[5]\n",
    "    body=body+divar_ads[6]\n",
    "    types=types+divar_ads[7]\n",
    "    urls=urls+divar_ads[8]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3c523b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.divar.ir/v/تالیسمان-۲۰۱۷_سواری-و-وانت_قزوین__دیوار/QZNhc_BL']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "32120a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'سفید'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r'\"title\":\"رنگ\",\"value\":\"([^\"]*)\"'\n",
    "internal_soup=str(internal_soup)\n",
    "match = re.search(pattern, internal_soup)\n",
    "match.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "971e3d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 5, 3, 2, 6, 7]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[2,4,5]\n",
    "b=[3,2,6,7]\n",
    "a=a+b\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e6f1ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
